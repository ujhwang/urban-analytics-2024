---
title: "Sentiment Analysis"
author: "Uijeong Hwang"
date: '2022-11-13'
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
---

<style type="text/css">
  body{
  font-family: Arial;
  }
</style>

```{r load-packages, include=FALSE}
# Package names
packages <- c("RedditExtractoR", "tidytext", "tidyverse", "textdata", "anytime", "magrittr", "wordcloud2",
              "ggdark", "syuzhet", "sentimentr", "lubridate", "here")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}

# Load packages
invisible(lapply(packages, library, character.only = TRUE))
```

Sentiment analysis involves utilizing Natural Language Processing (NLP), along with other related techniques, to identify and quantify affective states, typically from text data. Similar to advancements we've observed in computer vision, recent developments have introduced well-packaged models that are user-friendly and can be readily implemented out of the box. In this chapter, we will explore a range of models, from rudimentary to more advanced ones, to practice their application in diverse scenarios.

# 1. Rule-based Models

## 1-1. Bag-of-Words (or dictionary) model: `syuzhet` package

The Bag-of-Words (BoW) model is one of the simplest techniques used in NLP and text mining to represent and analyze text data. In BoW, a text (such as a sentence or document) is represented as an unordered set or "bag" of its words, disregarding grammar, word order, and any structural elements in the text.

```{r}
text_1 <- c("R is a very powerful tool for data analysis.",
             "I hate bugs in code. They are so annoying.",
             "I love programming in R! It’s so fun.")

# syuzhet package
get_sentiment(text_1, method='nrc')
get_sentiment(text_1, method='afinn')
get_sentiment(text_1, method='syuzhet')
get_sentiment(text_1, method='bing')
```

Using NRC sentiment dictionary, you can get the presence of eight different emotions and their valence.

```{r}
get_nrc_sentiment(text_1)
```

Wait, but if the models think 'hate' is a negative word and 'love' is a positive word, then how about negations? Do models consider 'not love' positive or negative?  Let's see the next example.

```{r}
text_2 <- c("I love programming in R! It’s fun.",
             "I don't love programming in R! It’s not fun.")

# syuzhet package
get_sentiment(text_2, method='nrc')
get_sentiment(text_2, method='afinn')
get_sentiment(text_2, method='syuzhet')
get_sentiment(text_2, method='bing')
```

This example above demonstrates the challenge of sentiment analysis that it involves understanding not only individual words but also the context in which they are used. When you run basic sentiment analysis that only uses a bag-of-words model, it can't understand the context and treats every word independently. So, the words "love" and "fun" in both sentences are contributing to the positive score, and the model doesn't consider the "not" which changes the meaning of those words.

## 1-2. Negation handling model: `sentimentr` package

Handling negation in NLP is crucial, as negation can entirely change the sentiment or meaning of a sentence. In sentiment analysis, the presence of a negation can reverse the polarity of a sentence.
`sentimentr` uses the same dictionary lookup approach as the previous model, but it also takes into account **valence shifters** (i.e., words that alter or intensify the polarity of words):

* **A negator** flips the sign of a polarized word:  
`r head(lexicon::hash_valence_shifters[y==1]$x, 10)`
* **An amplifier (intensifier)** increases the impact of a polarized word:  
`r head(lexicon::hash_valence_shifters[y==2]$x, 10)`
* **A de-amplifier (downtoner)** reduces the impact of a polarized word:  
`r head(lexicon::hash_valence_shifters[y==3]$x, 10)`
* **An adversative conjunction** overrules the previous clause containing a polarized word:  
`r head(lexicon::hash_valence_shifters[y==4]$x, 10)`

Let's run the model using example sentences.

```{r warning=F}
text_3 <- c("I love programming in R! It’s fun.",
             "I love programming in R so much! It's really fun.",
             "I don't love programming in R! It’s not fun.",
             "I don't love programming in R at all! It’s clearly not fun.")

# by sentence
sentiment(text_3)

# by string (a group of sentences)
sentiment_by(text_3)
```

### Sentiment Analysis on 2012 Presidential Debates
Let's apply this model to a dataset containing three presidential debates from the 2012 election, which is provided by the `sentimentr` package. First, we will take a look at the dataset. The `person` column includes the names of the two candidates and the three moderators.

```{r}
presidential_debates_2012[20:30,] ## %>% knitr::kable()
```

By applying the model to this dataset, we can determine who spoke more positively or negatively.

```{r}
debates_sentiment <- presidential_debates_2012 %>%
    mutate(dialogue_split = get_sentences(dialogue)) %$%
    sentiment_by(dialogue_split, list(person, time))

debates_sentiment %>% arrange(desc(ave_sentiment)) ## %>% knitr::kable()
```

Plot the distribution of sentiment per sentence.

```{r}
plot(debates_sentiment)
```

### Can the model handle sarcasm?

Let's test some sentences with "Bless your heart", a Southern phrase with different meanings depending on the context and tone.

```{r}
# 'Bless your heart' test

text_4 <- c(
  # Not sarcastic
  'I have heard that you have been through a lot lately. Bless your heart. Things will get better soon.',
  # Without 'bless your heart'
  'I have heard that you have been through a lot lately. Things will get better soon.',
  
  # Sarcastic
  'You really thought adding salt instead of sugar wouldn’t make a difference? Bless your heart.',
  # Without 'bless your heart'
  'You really thought adding salt instead of sugar wouldn’t make a difference?')

sentiment_by(text_4)
```

The result above highlights a clear limitation of the dictionary method -- it lacks an understanding of **context**. The model is unable to capture nuances or interpret meaning beyond predefined word associations.

  
# 2. Deep learning model

> In this section, we will employ a deep learning model; unfortunately, R is not the most conducive environment for training or testing such models. As expected, Python is the preferred option when it comes to deep learning due to its extensive libraries and community support. Similar to our approach in the computer vision module, we will utilize Google Colab ([link](https://colab.research.google.com/drive/1rynmZRcvB052PFaVQ2DsHrMYD1vyPN4Y?usp=sharing)). Upload (i.e., drag and drop) your Reddit data in csv format to your Colab session. If you have trouble getting Reddit data, you might as well use [this data](https://ujhwang.github.io/urban-analytics-2024/Lab/module_5/sample_reddit.csv): Reddit threads retrieved in November 2023 using a keyword "flu shot".

In NLP, the state-of-the-art model architecture is undoubtedly **Transformer** (Check out the benchmark [here](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary)). The architecture was introduced in 2017 and has since become a foundation for many NLP models. The Transformer architecture has been highly influential in NLP due to its ability to handle long-range dependencies in text and its scalability, allowing the training of large models that capture intricate language patterns and semantics.

Timeline of popular Transformer model releases ([source](https://huggingface.co/blog/bert-101#1-what-is-bert-used-for)):
![](transformer_timeline.png)

The script in Colab will demonstrate how to leverage a pre-trained **BERT** (Bidirectional Encoder Representations from Transformers) model to predict the sentiment of a given string. Developed by Google, BERT is a powerful language processing AI model designed for a range of NLP tasks.

BERT reads text and pays attention to the surrounding words to understand the context of each word. This is the “bidirectional” part, as it learns about the word based on the words that come before and after it, unlike older models which typically read text in one direction (left to right).

Thus, BERT helps computers better understand the **meaning of words** ***in a sentence***. Imagine you are learning a new language. When you hear or see a sentence, you don’t just look at each word in isolation—you look at the words around it to understand its meaning and context. If someone says, “It’s raining cats and dogs,” you understand that it’s an expression meaning it’s raining heavily, not that pets are falling from the sky.

Let's proceed to the [Colab](https://colab.research.google.com/drive/1rynmZRcvB052PFaVQ2DsHrMYD1vyPN4Y?usp=sharing) and analyze the sentiment of Reddit threads using the BERT model. Come back to this script once you complete running the code from Colab.

## 2-1. Import the result

Import the sentiment analysis result data processed from the Colab to R.

```{r}
# import the data
reddit_sentiment <- read_csv(????)

# drop NAs
reddit_sentiment %<>% drop_na('bert_label')
```

## 2-2. Comparison with the dictionary method

Get sentiment scores using the dictionary method for comparison.

```{r}
# Join thread title and text.
reddit_sentiment %<>%
  mutate(title = replace_na(title, ""),
         text = replace_na(text, ""),
         title_text = str_c(title, text, sep = ". "))

# dictionary method
reddit_sentiment_dictionary <- sentiment_by(reddit_sentiment$title_text)

reddit_sentiment$sentiment_dict <- reddit_sentiment_dictionary %>% pull(ave_sentiment)
reddit_sentiment$word_count <- reddit_sentiment_dictionary %>% pull(word_count)
```

Check the correlation between the sentiment values from two different methods.

```{r}
reddit_sentiment %<>% mutate(bert_label_numeric = str_sub(bert_label, 1, 1) %>% as.numeric())

cor(reddit_sentiment$bert_label_numeric, reddit_sentiment$sentiment_dict)
```

0.28 implies a mild positive correlation. However, in the scatter plot below, the two do not seem to be very correlated. The threads that got 4-5 stars from the BERT model are mostly below 0 (meaning negative) in the other method.

```{r}
ggplot(data = reddit_sentiment, aes(x = bert_label_numeric, y = sentiment_dict)) +
  geom_jitter(width = 0.1, height = 0) +
  geom_line(aes(y = 0), color = '#FFD700', lwd = 1, linetype='dashed') +
  dark_theme_grey()
```

> By the way, why did we use `geom_jitter` in the previous code block?

Let's look at some example threads and the predicted sentiment, and see which method makes more sense.

* BERT: 1 star (negative) vs. 5 stars (positive)

```{r}
bert_example <- reddit_sentiment %>%
  filter(bert_label_numeric %in% c(1,5)) %>%
  group_by(bert_label) %>%
  arrange(desc(bert_score)) %>%
  slice_head(n = 3) %>%
  ungroup()

# 1 star
bert_example %>% filter(bert_label_numeric == 1) %>% pull(title_text) %>% print()

# 5 star
bert_example %>% filter(bert_label_numeric == 5) %>% pull(title_text) %>% print()
```

* Dictionary method: negative vs. positive

```{r}
sentimentr_example <- reddit_sentiment %>%
  mutate(sentimentr_abs = abs(sentiment_dict),
         sentimentr_binary = case_when(sentiment_dict > 0 ~ 'positive',
                                       TRUE ~ 'negative')) %>%
  group_by(sentimentr_binary) %>%
  arrange(desc(sentimentr_abs)) %>%
  slice_head(n = 3) %>%
  ungroup() %>%
  arrange(sentiment_dict)

# negative
sentimentr_example %>% filter(sentimentr_binary == 'negative') %>% pull(title_text) %>% print()

# positive
sentimentr_example %>% filter(sentimentr_binary == 'positive') %>% pull(title_text) %>% print()
```


# 3. Visualization

Let's visualize the sentiment analysis result from the BERT model.

## 3-1: Sentiment distribution

* Number of threads by sentiment category.

```{r}
reddit_sentiment %>%
  ggplot(aes(x = bert_label)) +
  geom_bar(fill = "white") +
  dark_theme_gray()
```

* Word counts by sentiment category.

```{r}
reddit_sentiment %>%
  ggplot(aes(x = bert_label, y = ????)) +
  geom_jitter(height = 0, width = 0.05) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.4, color = "red") +
  dark_theme_gray()
```

* Association between a thread's sentiment and the number of comments on the thread.

```{r warning=F}
# Remove outliers
reddit_sentiment_rm_outlier <- reddit_sentiment %>%
  group_by(bert_label) %>%
  filter(
    between(
      comments,
      quantile(comments, 0.25) - 1.5 * IQR(comments),
      quantile(comments, 0.75) + 1.5 * IQR(comments)))

# Correlation analysis
cor.test(reddit_sentiment_rm_outlier$comments, reddit_sentiment_rm_outlier$bert_label_numeric)

# Scatterplot
reddit_sentiment_rm_outlier %>%
  ggplot(aes(x = bert_label_numeric, y = ????)) +
  geom_jitter(height = 0, width = 0.05) + 
  geom_smooth(method = 'loess', span = 0.75) +
  dark_theme_gray()

```

## 3-2: Word clouds

Using word clouds, we can visualize words that are frequently seen in either positive or negative threads. Using the same code from the previous Rmd file, we tokenize the text and remove stop words.

```{r}
# Stop word removal and tokenization
data("stop_words")
replace_reg <- "http[s]?://[A-Za-z\\d/\\.]+|&amp;|&lt;|&gt;"

reddit_sentiment_clean <- reddit_sentiment %>%
  mutate(title_text = str_replace_all(title_text, replace_reg, "")) %>%
  # tokenize
  ????(word, title_text, token = ????) %>%
  # remove stop words
  ????(stop_words, by = "word") %>%
  filter(str_detect(word, "[a-z]")) %>%
  filter(!word %in% c('flu','shot','shots')) # You need to replace this with your keyword
```

We are not interested in words that are commonly seen in both positive and negative threads. We can identify words that are uniquely seen in either positive or negative threads using `anti_join`.

```{r}
# negative text
reddit_sentiment_clean_negative <- reddit_sentiment_clean %>%
  filter(bert_label_numeric %in% c(1,2))
# positive text
reddit_sentiment_clean_positive <- reddit_sentiment_clean %>%
  filter(bert_label_numeric %in% c(4,5))

# Remove words that are commonly seen in both negative and positive threads
reddit_sentiment_clean_negative_unique <- reddit_sentiment_clean_negative %>%
  anti_join(reddit_sentiment_clean_positive, by = 'word')
reddit_sentiment_clean_positive_unique <- reddit_sentiment_clean_positive %>%
  anti_join(reddit_sentiment_clean_negative, by = 'word')
```

* Words appearing in negative threads
```{r}
# Wordcloud with a custom color palette
n <- 20
h <- runif(n, 0, 1) # any color
s <- runif(n, 0.6, 1) # vivid
v <- runif(n, 0.3, 0.7) # neither too dark nor too bright

df_hsv <- data.frame(h = h, s = s, v = v)
pal <- apply(df_hsv, 1, function(x) hsv(x['h'], x['s'], x['v']))
pal <- c(pal, rep("grey", 10000))

reddit_sentiment_clean_negative_unique %>%
  count(word, sort = TRUE) %>%
  ????(color = pal,
       minRotation = -pi/6,
       maxRotation = -pi/6,
       rotateRatio = 1)
```

* Words appearing in positive threads

```{r}
# Wordcloud with a custom color palette
n <- 20
h <- runif(n, 0, 1) # any color
s <- runif(n, 0.6, 1) # vivid
v <- runif(n, 0.3, 0.7) # neither too dark nor too bright

df_hsv <- data.frame(h = h, s = s, v = v)
pal <- apply(df_hsv, 1, function(x) hsv(x['h'], x['s'], x['v']))
pal <- c(pal, rep("grey", 10000))

reddit_sentiment_clean_positive_unique %>%
  count(word, sort = TRUE) %>%
  ????(color = pal,
       minRotation = pi/6,
       maxRotation = pi/6,
       rotateRatio = 1)
```

## 3-3: Temporal analysis

First, create new columns for the temporal analysis: `date, year, day_of_week, is_weekend, time`
```{r}
reddit_sentiment %<>%
  mutate(date = as.POSIXct(date_utc)) %>%
  filter(!is.na(date)) %>%
  mutate(year = year(date),
         day_of_week = wday(date, label = TRUE),
         is_weekend = ifelse(day_of_week %in% c("Sat", "Sun"), "Weekend", "Weekday"),
         time = timestamp %>%
           anytime(tz = anytime:::getTZ()) %>%
           str_split('-| |:') %>%
           sapply(function(x) as.numeric(x[4])))
```

* Sentiment by year using a stacked bar plot

```{r}
# sentiment by year
reddit_sentiment %>%
  ggplot(aes(x = ????, fill = bert_label)) +
  geom_????(position = 'stack') +
  scale_x_continuous(breaks = seq(min(reddit_sentiment$year),
                                  max(reddit_sentiment$year),
                                  by = 1)) +
  scale_fill_brewer(palette = 'PuRd', direction = -1) +
  dark_theme_grey()
```
  
* Set `position = 'fill'` to see the proportions.

```{r}
# sentiment by year
reddit_sentiment %>%
  ggplot(aes(x = ????, fill = bert_label)) +
  geom_????(position = ????) +
  scale_x_continuous(breaks = seq(min(reddit_sentiment$year),
                                  max(reddit_sentiment$year),
                                  by = 1)) +
  scale_fill_brewer(palette = 'PuRd', direction = -1) +
  dark_theme_grey()
```

* Sentiment by day of week.

```{r}
# sentiment by day
reddit_sentiment %>%
  ggplot(aes(x = ????, fill = bert_label)) +
  geom_????(position = ????) +
  scale_fill_brewer(palette = 'PuRd', direction = -1) +
  dark_theme_grey()
```

* Sentiment by time of day.

```{r}
reddit_sentiment %>%
  ggplot(aes(x = ????, fill = bert_label)) +
  geom_histogram(bins = 24, position = 'fill', color = 'black', lwd = 0.2) +
  scale_x_continuous(breaks = seq(0, 24, by=1)) +
  scale_fill_manual(values = c('#bc5090', '#bc5090', '#ff6361', '#ffa600', '#ffa600')) +
  dark_theme_grey()
```





